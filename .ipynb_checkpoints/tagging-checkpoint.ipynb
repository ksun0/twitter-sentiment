{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import *\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import sys\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# already ran script to generate .txt files\n",
    "# empty descriptions are null ('\\xa0')\n",
    "\n",
    "# with open('KynData/news.json') as data_file:    \n",
    "#     news_data = json.load(data_file)\n",
    "# data_file.close()\n",
    "\n",
    "# news_file = open('KynData/news.txt','w') \n",
    "# news_file.write('\\n\\n|| ')\n",
    "# for i in range(len(news_data)):\n",
    "#     try:\n",
    "#         news_file.write(news_data[i]['description'] + '\\n\\n|| ')\n",
    "# #         if(len(news_data[i]['description']) < 2):\n",
    "# #             print(news_data[i])\n",
    "#     except:\n",
    "#         print(\"This object does not have a description:\\n\", news_data[i])\n",
    "# news_file.close() \n",
    "\n",
    "\n",
    "# with open('KynData/news_flagged.json') as data_file:    \n",
    "#     news_data = json.load(data_file)\n",
    "# data_file.close()\n",
    "\n",
    "# news_file = open('KynData/news_flagged.txt','w') \n",
    "# news_file.write('\\n\\n|| ')\n",
    "# for i in range(len(news_data)):\n",
    "#     try:\n",
    "#         news_file.write(news_data[i]['description'] + '\\n\\n|| ')\n",
    "#     except:\n",
    "#         print(\"This object does not have a description:\\n\", news_data[i])\n",
    "# news_file.close() \n",
    "\n",
    "\n",
    "# with open('KynData/funding.json') as data_file:    \n",
    "#     funding_data = json.load(data_file)\n",
    "# data_file.close()\n",
    "\n",
    "# funding_file = open('KynData/funding.txt','w') \n",
    "# funding_file.write('\\n\\n|| ')\n",
    "# for i in range(len(funding_data)):\n",
    "#     try:\n",
    "#         funding_file.write(funding_data[i]['description'] + '\\n\\n|| ')\n",
    "#     except:\n",
    "#         print(\"This object does not have a description:\\n\", funding_data[i])\n",
    "# funding_file.close() \n",
    "\n",
    "\n",
    "# with open('KynData/events.json') as data_file:    \n",
    "#     events_data = json.load(data_file)\n",
    "# data_file.close()\n",
    "\n",
    "# events_file = open('KynData/events.txt','w') \n",
    "# events_file.write('\\n\\n|| ')\n",
    "# for i in range(len(events_data)):\n",
    "#     try:\n",
    "#         events_file.write(events_data[i]['description'] + '\\n\\n|| ')\n",
    "#     except:\n",
    "#         print(\"This object does not have a description:\\n\", events_data[i])\n",
    "# events_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLists Files: ['.DS_Store', 'companies.json', 'companies.txt', 'custom_stopwords.txt', 'discussions.json', 'discussions.txt', 'events.json', 'events.txt', 'funding.json', 'funding.txt', 'news.json', 'news.txt', 'news2.txt', 'news_tags copy 2.txt', 'news_tags copy.txt', 'news_tags.txt']\n",
      "\n",
      "-------------------\n",
      "\n",
      "WordLists Type: <class 'nltk.corpus.reader.plaintext.PlaintextCorpusReader'>\n",
      "wordlists.words() Type: <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "wordlists.paras() Type: <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "wordlists.paras()[0] Type: <class 'list'>\n",
      "\n",
      "-------------------\n",
      "\n",
      "Text Type: <class 'nltk.text.Text'>\n",
      "['||', 'As', 'bees', 'slip', 'onto', 'the', 'endangered', 'species', 'lists', ',', 'researchers', 'in', 'Japan', 'are', 'pollinating', 'lilies', 'with', 'insect', '-', 'sized']\n",
      "[('.', 3028), ('the', 3020), (',', 2553), ('of', 2314), ('to', 1877), ('||', 1837), ('a', 1822), ('and', 1609), ('in', 1270), ('-', 1258), ('that', 813), ('for', 740), ('have', 583), ('is', 558), ('The', 492), ('new', 478), (\"'\", 477), ('s', 422), ('with', 405), ('from', 386), ('on', 374), ('are', 369), ('has', 360), ('by', 351), ('as', 329), ('an', 327), ('be', 301), ('A', 296), ('can', 272), ('energy', 272), ('could', 254), ('at', 251), ('’', 233), ('it', 230), ('researchers', 221), ('--', 187), ('Researchers', 186), ('solar', 182), ('first', 179), ('or', 178), ('more', 176), ('(', 170), ('been', 169), ('developed', 167), ('which', 166), ('its', 164), ('this', 156), ('than', 156), ('into', 156), ('scientists', 150)]\n",
      "\n",
      "-------------------\n",
      "\n",
      "Brown University Dataset Type: <class 'nltk.corpus.reader.tagged.CategorizedTaggedCorpusReader'>\n",
      "\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "corpus_root = '/Users/ksun/downloads/csc630/jupyter/kynplex/KynData'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "print('WordLists Files:', wordlists.fileids())\n",
    "print('\\n-------------------\\n')\n",
    "print('WordLists Type:', type(wordlists))\n",
    "print('wordlists.words() Type:', type(wordlists.words('news.txt')))\n",
    "print('wordlists.paras() Type:', type(wordlists.paras('news.txt')))\n",
    "print('wordlists.paras()[0] Type:', type(wordlists.paras('news.txt')[0]))\n",
    "# working with wordlists is a bit sketch, just parse raw text and convert to Text object\n",
    "print('\\n-------------------\\n')\n",
    "text = nltk.Text(wordlists.words('news.txt'))\n",
    "print('Text Type:', type(text))\n",
    "print(text[:20])\n",
    "fdist1 = FreqDist(text)\n",
    "print(fdist1.most_common(50))\n",
    "print('\\n-------------------\\n')\n",
    "print('Brown University Dataset Type:', type(brown))\n",
    "print('\\n-------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0767212897684\n"
     ]
    }
   ],
   "source": [
    "# very messy, I can find indexes of new line and do brown.words()[0:10]\n",
    "# or convert article into one utf-8 string and use word_tokenize\n",
    "# 0.077\n",
    "# TESTED WITH TIME, TAKES MUCH LONGER\n",
    "\n",
    "def tfidf(term, article, corpus): # term is specific word, article is one paragraph in corpus (description), \n",
    "                                  # corpus is wordlists.paras('news.txt')\n",
    "    words = [word for sentence in article for word in sentence] # flattened article\n",
    "    tf = words.count(term) / (len(words))\n",
    "    count = 0\n",
    "    for i in range(len(corpus)):\n",
    "        words = [word for sentence in corpus[i] for word in sentence]\n",
    "        if words.count(term) > 0:\n",
    "            count += 1\n",
    "    idf = np.log((len(corpus)) / count) # 1838 paragraphs\n",
    "    return tf * idf\n",
    "\n",
    "for i in range(len(wordlists.paras('news.txt'))): # iterate through every paragraph\n",
    "    print(tfidf('bees', wordlists.paras('news.txt')[i], wordlists.paras('news.txt')))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has 0.0767883847182\n",
      "in 0.0673746077112\n",
      "that 0.0464359423331\n",
      "to 0.0228331264337\n",
      "colonies 0.0\n",
      "on 0.0\n",
      "put 0.0\n",
      "demand 0.0\n",
      "agricultural 0.0\n",
      "modern 0.0\n",
      "burden 0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# We're adding some on our own - could be done inline like this...\n",
    "# custom_stopwords = set((u'–', u'dass', u'mehr'))\n",
    "# ... but let's read them from a file instead (one stopword per line, UTF-8)\n",
    "custom_stopwords = set(codecs.open('KynData/custom_stopwords.txt', 'r', 'utf-8').read().splitlines())\n",
    "\n",
    "stopwords = default_stopwords | custom_stopwords # might be some duplicates: http://www.ranks.nl/stopwords\n",
    "\n",
    "f = open('KynData/news.txt')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "raw_words = word_tokenize(raw)\n",
    "# np_raw_words = np.array(raw_words)\n",
    "# breaks = np.where(np_raw_words == '||')[0] # 1837 breaks, 1836 paragraphs\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "words = [word for word in raw_words if len(word) > 1]\n",
    "\n",
    "np_words = np.array(words)\n",
    "breaks = np.where(np_words == '||')[0] # still 1837 breaks, so 1836 paragraphs\n",
    "\n",
    "def num_articles_containing(term, text):\n",
    "    count = 0\n",
    "    for i in range(len(breaks) - 1):\n",
    "        fdist = FreqDist(text[breaks[i]+1:breaks[i+1]])\n",
    "        if fdist[term] > 0:\n",
    "            count+=1\n",
    "    return count\n",
    "        \n",
    "def tfidf(term, article, text): # term is specific word, article is one paragraph in text (description), \n",
    "                                # text is Text(raw_words) or words\n",
    "    fdist = FreqDist(article)\n",
    "    tf = fdist[term] / len(article)\n",
    "    count = num_articles_containing(term, text)\n",
    "    idf = np.log((len(breaks) - 1) / count)\n",
    "    tfidf = tf * idf\n",
    "    return tf * idf\n",
    "\n",
    "matrix = {}\n",
    "for w in range(breaks[0]+1, breaks[1]):\n",
    "    matrix[words[w]] = tfidf(words[w], words[breaks[5]+1:breaks[6]], words)\n",
    "\n",
    "key1 = sorted(matrix, key=matrix.__getitem__)[-1]\n",
    "key2 = sorted(matrix, key=matrix.__getitem__)[-2]\n",
    "key3 = sorted(matrix, key=matrix.__getitem__)[-3]\n",
    "key4 = sorted(matrix, key=matrix.__getitem__)[-4]\n",
    "key5 = sorted(matrix, key=matrix.__getitem__)[-5]\n",
    "key6 = sorted(matrix, key=matrix.__getitem__)[-6]\n",
    "key7 = sorted(matrix, key=matrix.__getitem__)[-7]\n",
    "key8 = sorted(matrix, key=matrix.__getitem__)[-8]\n",
    "key9 = sorted(matrix, key=matrix.__getitem__)[-9]\n",
    "key10 = sorted(matrix, key=matrix.__getitem__)[-10]\n",
    "key11 = sorted(matrix, key=matrix.__getitem__)[-11]\n",
    "# key12 = sorted(matrix, key=matrix.__getitem__)[-12]\n",
    "# key13 = sorted(matrix, key=matrix.__getitem__)[-13]\n",
    "# key14 = sorted(matrix, key=matrix.__getitem__)[-14]\n",
    "# key15 = sorted(matrix, key=matrix.__getitem__)[-15]\n",
    "# key16 = sorted(matrix, key=matrix.__getitem__)[-16]\n",
    "# key17 = sorted(matrix, key=matrix.__getitem__)[-17]\n",
    "# key18 = sorted(matrix, key=matrix.__getitem__)[-18]\n",
    "# key19 = sorted(matrix, key=matrix.__getitem__)[-19]\n",
    "print(key1, str(matrix.get(key1)))\n",
    "print(key2, str(matrix.get(key2)))\n",
    "print(key3, str(matrix.get(key3)))\n",
    "print(key4, str(matrix.get(key4)))\n",
    "print(key5, str(matrix.get(key5)))\n",
    "print(key6, str(matrix.get(key6)))\n",
    "print(key7, str(matrix.get(key7)))\n",
    "print(key8, str(matrix.get(key9)))\n",
    "print(key9, str(matrix.get(key9)))\n",
    "print(key10, str(matrix.get(key10)))\n",
    "print(key11, str(matrix.get(key11)))\n",
    "# print(key12, str(matrix.get(key12)))\n",
    "# print(key13, str(matrix.get(key13)))\n",
    "# print(key14, str(matrix.get(key14)))\n",
    "# print(key15, str(matrix.get(key15)))\n",
    "# print(key16, str(matrix.get(key16)))\n",
    "# print(key17, str(matrix.get(key17)))\n",
    "# print(key18, str(matrix.get(key18)))\n",
    "# print(key19, str(matrix.get(key19)))\n",
    "\n",
    "\n",
    "# tag_file = open('KynData/news_tags.txt', 'w')\n",
    "\n",
    "# for p in range(len(breaks) - 1): # iterate through every paragraph\n",
    "#     matrix = {}\n",
    "#     for w in range(breaks[p]+1, breaks[p+1]):\n",
    "#         matrix[words[w]] = tfidf(words[w], words[breaks[p]+1:breaks[p+1]], words)\n",
    "#     try:\n",
    "#         key1 = sorted(matrix, key=matrix.__getitem__)[-1]\n",
    "#         key2 = sorted(matrix, key=matrix.__getitem__)[-2]\n",
    "#         key3 = sorted(matrix, key=matrix.__getitem__)[-3]\n",
    "#         key4 = sorted(matrix, key=matrix.__getitem__)[-4]\n",
    "#         key5 = sorted(matrix, key=matrix.__getitem__)[-5]\n",
    "#         print(key1, str(matrix.get(key1)))\n",
    "#         print(key2, str(matrix.get(key2)))\n",
    "#         print(key3, str(matrix.get(key3)))\n",
    "#         print(key4, str(matrix.get(key4)))\n",
    "#         print(key5, str(matrix.get(key5)))\n",
    "#         print('------------------')\n",
    "#         tag_file.write(key1)\n",
    "#         tag_file.write(' | ')\n",
    "#         tag_file.write(str(matrix.get(key1)))\n",
    "#         tag_file.write('\\n')\n",
    "#         tag_file.write(key2)\n",
    "#         tag_file.write(' | ')\n",
    "#         tag_file.write(str(matrix.get(key2)))\n",
    "#         tag_file.write('\\n')\n",
    "#         tag_file.write(key3)\n",
    "#         tag_file.write(' | ')\n",
    "#         tag_file.write(str(matrix.get(key3)))\n",
    "#         tag_file.write('\\n')\n",
    "#         tag_file.write(key4)\n",
    "#         tag_file.write(' | ')\n",
    "#         tag_file.write(str(matrix.get(key4)))\n",
    "#         tag_file.write('\\n')\n",
    "#         tag_file.write(key5)\n",
    "#         tag_file.write(' | ')\n",
    "#         tag_file.write(str(matrix.get(key5)))\n",
    "#         tag_file.write('\\n\\n------------------------\\n\\n')\n",
    "#     except:\n",
    "#         print(\"Error, perhaps empty description or not enough words in description?\")\n",
    "#         print('------------------')\n",
    "#         tag_file.write(\"Error, perhaps empty description or not enough words in description?\")\n",
    "#         tag_file.write('\\n\\n------------------------\\n\\n')\n",
    "# tag_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d1d5feee9ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdefault_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "f = open('KynData/news.txt', 'r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "words = word_tokenize(raw)\n",
    "\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "custom_stopwords = set(codecs.open('KynData/custom_stopwords.txt', 'r', 'utf-8').read().splitlines())\n",
    "\n",
    "special_words = set(codecs.open('KynData/good_words.txt', 'r', 'utf-8').read().splitlines())\n",
    "\n",
    "# Lemmatize\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "words = [wnl.lemmatize(w) for w in words]\n",
    "\n",
    "# INSTEAD, PUT ALL RESULTS INTO STOPWORD LIST, DON'T PREPROCESS LIKE THIS\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "single_stopwords = set([word for word in words if len(word) < 2])\n",
    "# Remove numbers\n",
    "num_stopwords = set([word for word in words if word.isnumeric()])\n",
    "\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "proper_nouns = set([p[0] for p in tagged_words if p[1] == 'NNP'])\n",
    "\n",
    "verbs = set([v[0] for v in tagged_words if v[1][0] == 'V' and v[0] not in special_words])\n",
    "cardinal_conjunctions = set([c[0] for c in tagged_words if c[1][0] == 'C' and c[0] not in special_words])\n",
    "\n",
    "stopwords = default_stopwords | custom_stopwords | single_stopwords | num_stopwords | verbs | cardinal_conjunctions\n",
    "stopwords = list(stopwords)\n",
    "\n",
    "raw = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in words]).strip()\n",
    "\n",
    "paragraphs = [p for p in raw.split('||')]\n",
    "paragraphs = paragraphs[1:-1]\n",
    "\n",
    "# print(len(paragraphs), '\\n---------------\\n')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(strip_accents='ascii', analyzer='word', ngram_range=(1,1), min_df = 0, \n",
    "                     stop_words = stopwords, lowercase = True)\n",
    "tfidf_matrix =  tf.fit_transform(paragraphs)\n",
    "feature_names = tf.get_feature_names() \n",
    "\n",
    "# print(len(feature_names), '\\n---------------\\n')\n",
    "# print(feature_names[50:70], '\\n---------------\\n')\n",
    "\n",
    "dense = tfidf_matrix.todense()\n",
    "# print(len(dense[0].tolist()[0]), '\\n---------------\\n')\n",
    "\n",
    "tag_file = open('KynData/news_tags.txt', 'w')\n",
    "\n",
    "for i in range(len(paragraphs)):\n",
    "    print('---------------\\nParagraph', i, ':', file=tag_file)\n",
    "    episode = dense[i].tolist()[0]\n",
    "    phrase_scores = [pair for pair in zip(range(0, len(episode)), episode) if pair[1] > 0]\n",
    "    print(phrase_scores)\n",
    "    # print(len(phrase_scores), '\\n---------------\\n')\n",
    "\n",
    "    # print(sorted(phrase_scores, key=lambda t: t[1] * -1)[:5], '\\n---------------\\n')\n",
    "\n",
    "    sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)\n",
    "\n",
    "    for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:10]:\n",
    "        print('{0: <25} {1}'.format(phrase, score), file=tag_file)\n",
    "        \n",
    "tag_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/anabranch/48c5c0124ba4e162b2e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Tags for better search and more subcategories for ONE category\n",
    "# TODO: now combine tfidf with machine learning (scikit learn), cosine similarity with sci-kit learn and training\n",
    "# TODO: do stuff with bigrams?\n",
    "# TODO: better stopwords, High DF, Low IDF, remove verbs\n",
    "# TODO: give priority to proper nouns?\n",
    "# TODO: POS tagging is nor perfect. Train custom tagger?\n",
    "# TODO: perhaps do ML to find a proper tfidf threshold\n",
    "# TODO: should i pass in raw or filtered? Can i preprocess? CAN I REMOVE VERBS BEFORE PASSING INTO ALGORITHM?\n",
    "    \n",
    "# Step 2\n",
    "# TODO: Naive Bayes to classify into flagged or not flagged\n",
    "\n",
    "# Step 3: Classifying into ONE category\n",
    "# TODO: categorize with k-means clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
